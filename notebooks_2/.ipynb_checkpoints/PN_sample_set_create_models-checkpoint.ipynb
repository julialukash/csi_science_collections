{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import artm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "import distances_helper as dh \n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths\n",
    "from print_helper import PrintHelper\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from numpy.linalg import norm as euclidean_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\ndw_model5\\np_19_12\\models.txt\n",
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\..\\data\\postnauka\\UCI_collections\\ndw_model5\n",
      "ndw_model5\n",
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\..\\data\\postnauka\\bigARTM_files\\ndw_model5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigPaths('config_sample.cfg')\n",
    "original_model_name = 'phi_model5'\n",
    "plot_maker = PlotMaker()\n",
    "printer = PrintHelper()\n",
    "print config.models_file_name\n",
    "print config.dataset_path\n",
    "print config.dataset_folder_name\n",
    "print config.output_batches_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_file = open(config.models_file_name, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(current_dictionary, n_topics, n_doc_passes, seed_value, n_top_tokens, p_mass_threshold):    \n",
    "    print '[{}] creating model'.format(datetime.now())\n",
    "    model = artm.ARTM(num_topics=n_topics, dictionary=current_dictionary, cache_theta=True, seed=seed_value, \n",
    "                  class_ids={'@default_class': 1.0})\n",
    "    model.num_document_passes = n_doc_passes\n",
    "    add_scores_to_model(model, n_top_tokens=n_top_tokens, p_mass_threshold=p_mass_threshold)\n",
    "    return model\n",
    "def add_scores_to_model(artm_model, n_top_tokens, p_mass_threshold):\n",
    "    print '[{}] adding scores'.format(datetime.now())\n",
    "    artm_model.scores.add(artm.PerplexityScore(name='perplexity_score', dictionary=dictionary))\n",
    "    artm_model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score', class_id='@default_class'))\n",
    "    artm_model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))\n",
    "    artm_model.scores.add(artm.TopicKernelScore(name='topic_kernel_score', class_id='@default_class', \n",
    "                                                probability_mass_threshold=p_mass_threshold))\n",
    "    artm_model.scores.add(artm.TopTokensScore(name='top_tokens_score', class_id='@default_class', num_tokens=n_top_tokens))\n",
    "def fit_one_model(model, _n_iterations, _model_name=''): \n",
    "    print '[{}] fitting'.format(datetime.now())\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=_n_iterations)\n",
    "    print '[{}] outputting'.format(datetime.now())\n",
    "    printer.print_artm_model(model, _model_name, _n_iterations, output_file=models_file)\n",
    "    model_pics_file_name =  path.join(config.experiment_path, _model_name)\n",
    "    plot_maker.make_tm_plots(model, model_pics_file_name)\n",
    "    model_output_file_name = path.join(config.experiment_path, _model_name + '.txt')\n",
    "    printer.print_scores(model, _model_name, _n_iterations, model_output_file_name)\n",
    "    printer.print_top_tokens(model, model_output_file_name)\n",
    "    return model\n",
    "def save_pickle_file(dists, filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'wb')\n",
    "    pickle.dump(dists, pickle_file)\n",
    "    pickle_file.close()\n",
    "def load_pickle_file(filename, _path):\n",
    "    pickle_filename = path.join(_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'rb')\n",
    "    p_file = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    return p_file\n",
    "def save_model_pickle(_model_name, _model, _save=True):\n",
    "    phi = _model.get_phi()\n",
    "    phi = phi[(phi.T != 0).any()]\n",
    "    theta = _model.get_theta()    \n",
    "    saved_top_tokens = _model.score_tracker['top_tokens_score'].last_tokens\n",
    "    if _save:\n",
    "        save_pickle_file(phi, 'phi_{}.p'.format(_model_name))\n",
    "        save_pickle_file(theta, 'theta_{}.p'.format(_model_name))\n",
    "        save_pickle_file(saved_top_tokens, 'saved_top_tokens_{}.p'.format(_model_name))\n",
    "    return phi, theta, saved_top_tokens\n",
    "def load_model_pickle(_model_name, _distance_name, _path=config.experiment_path):\n",
    "    phi = load_pickle_file('phi_{}.p'.format(_model_name), _path)\n",
    "    theta = load_pickle_file('theta_{}.p'.format(_model_name), _path)\n",
    "    saved_top_tokens = load_pickle_file('saved_top_tokens_{}.p'.format(_model_name), _path)\n",
    "    distances = load_pickle_file('{}.p'.format(_distance_name), _path)\n",
    "    return phi, theta, saved_top_tokens, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=config.output_batches_path,\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path=config.dictionary_path + '.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 16:32:23.798000] creating model\n",
      "[2016-12-19 16:32:25.109000] adding scores\n",
      "[2016-12-19 16:32:25.125000] fitting\n",
      "[2016-12-19 17:03:24.270000] outputting\n",
      "name = model1, n_topics = 500, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=500, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model1')\n",
    "model1 = tmp_model; tmp_model = None\n",
    "phi1, theta1, saved_top_tokens1 = save_model_pickle('model1', model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 17:03:51.357000] creating model\n",
      "[2016-12-19 17:03:52.678000] adding scores\n",
      "[2016-12-19 17:03:52.795000] fitting\n",
      "[2016-12-19 17:39:09.967000] outputting\n",
      "name = model2, n_topics = 500, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -0.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=500, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.05\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model2')\n",
    "model2 = tmp_model; tmp_model = None\n",
    "phi2, theta2, saved_top_tokens2 = save_model_pickle('model2', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 18:23:37.275000] creating model\n",
      "[2016-12-19 18:23:38.217000] adding scores\n",
      "[2016-12-19 18:23:38.217000] fitting\n",
      "[2016-12-19 18:29:45.533000] outputting\n",
      "name = model3, n_topics = 50, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=50, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model3')\n",
    "model3 = tmp_model; tmp_model = None\n",
    "phi3, theta3, saved_top_tokens3 = save_model_pickle('model3', model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 18:40:18.360000] creating model\n",
      "[2016-12-19 18:40:19.373000] adding scores\n",
      "[2016-12-19 18:40:19.373000] fitting\n",
      "[2016-12-19 18:45:53.723000] outputting\n",
      "name = model4, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model4')\n",
    "model4 = tmp_model; tmp_model = None\n",
    "phi4, theta4, saved_top_tokens4 = save_model_pickle('model4', model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 17:40:31.787000] creating model\n",
      "[2016-12-19 17:40:34.055000] adding scores\n",
      "[2016-12-19 17:40:34.227000] fitting\n",
      "[2016-12-19 18:18:51.675000] outputting\n",
      "name = model5, n_topics = 500, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -0.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=500, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.05\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model5')\n",
    "model5 = tmp_model; tmp_model = None\n",
    "phi5, theta5, saved_top_tokens5 = save_model_pickle('model5', model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 20:07:59.923000] creating model\n",
      "[2016-12-19 20:08:01.357000] adding scores\n",
      "[2016-12-19 20:08:01.408000] fitting\n",
      "[2016-12-19 20:12:17.817000] outputting\n",
      "name = model6, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -0.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.05\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model6')\n",
    "model6 = tmp_model; tmp_model = None\n",
    "phi6, theta6, saved_top_tokens6 = save_model_pickle('model6', model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 20:14:52.651000] creating model\n",
      "[2016-12-19 20:14:53.671000] adding scores\n",
      "[2016-12-19 20:14:53.677000] fitting\n",
      "[2016-12-19 20:19:23.098000] outputting\n",
      "name = model7, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model7')\n",
    "model7 = tmp_model; tmp_model = None\n",
    "phi7, theta7, saved_top_tokens7 = save_model_pickle('model7', model7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 20:38:18.228000] creating model\n",
      "[2016-12-19 20:38:19.142000] adding scores\n",
      "[2016-12-19 20:38:19.157000] fitting\n",
      "[2016-12-19 20:41:54.015000] outputting\n",
      "name = model8, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.2\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model8')\n",
    "model8 = tmp_model; tmp_model = None\n",
    "phi8, theta8, saved_top_tokens8 = save_model_pickle('model8', model8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 01:13:18.300000] creating model\n",
      "[2016-12-20 01:13:19.285000] adding scores\n",
      "[2016-12-20 01:13:19.304000] fitting\n",
      "[2016-12-20 01:17:43.834000] outputting\n",
      "name = model9, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.2\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model9')\n",
    "model9 = tmp_model; tmp_model = None\n",
    "phi9, theta9, saved_top_tokens9 = save_model_pickle('model9', model9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 00:59:59.526000] creating model\n",
      "[2016-12-20 01:00:00.615000] adding scores\n",
      "[2016-12-20 01:00:00.700000] fitting\n",
      "[2016-12-20 01:04:15.469000] outputting\n",
      "name = model10, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -100\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.2\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model10')\n",
    "model10 = tmp_model; tmp_model = None\n",
    "phi10, theta10, saved_top_tokens10 = save_model_pickle('model10', model10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 01:20:39.961000] creating model\n",
      "[2016-12-20 01:20:40.962000] adding scores\n",
      "[2016-12-20 01:20:40.970000] fitting\n",
      "[2016-12-20 01:25:30.249000] outputting\n",
      "name = model11, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model11')\n",
    "model11 = tmp_model; tmp_model = None\n",
    "phi11, theta11, saved_top_tokens11 = save_model_pickle('model11', model11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 01:27:42.571000] creating model\n",
      "[2016-12-20 01:27:43.568000] adding scores\n",
      "[2016-12-20 01:27:43.583000] fitting\n",
      "[2016-12-20 01:32:31.449000] outputting\n",
      "name = model12, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 10000\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model12')\n",
    "model12 = tmp_model; tmp_model = None\n",
    "phi12, theta12, saved_top_tokens12 = save_model_pickle('model12', model12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 01:33:57.864000] creating model\n",
      "[2016-12-20 01:33:58.872000] adding scores\n",
      "[2016-12-20 01:33:58.872000] fitting\n",
      "[2016-12-20 01:37:54.329000] outputting\n",
      "name = model13, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -15\n",
      "decorrelator_phi_regularizer, tau = 10000\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -15\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model13')\n",
    "model13 = tmp_model; tmp_model = None\n",
    "phi13, theta13, saved_top_tokens13 = save_model_pickle('model13', model13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 01:42:31.101000] creating model\n",
      "[2016-12-20 01:42:32.108000] adding scores\n",
      "[2016-12-20 01:42:32.177000] fitting\n",
      "[2016-12-20 01:45:44.032000] outputting\n",
      "name = model14, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 1000000\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model14')\n",
    "model14 = tmp_model; tmp_model = None\n",
    "phi14, theta14, saved_top_tokens14 = save_model_pickle('model14', model14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 01:53:49.251000] creating model\n",
      "[2016-12-20 01:53:50.046000] adding scores\n",
      "[2016-12-20 01:53:50.054000] fitting\n",
      "[2016-12-20 01:56:35.309000] outputting\n",
      "name = model15, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 10000\n",
      "ss_phi_regularizer, tau = -10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -10\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model15')\n",
    "model15 = tmp_model; tmp_model = None\n",
    "phi15, theta15, saved_top_tokens15 = save_model_pickle('model15', model15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 02:00:24.154000] creating model\n",
      "[2016-12-20 02:00:24.960000] adding scores\n",
      "[2016-12-20 02:00:24.976000] fitting\n",
      "[2016-12-20 02:03:14.014000] outputting\n",
      "name = model16, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 10000\n",
      "ss_phi_regularizer, tau = -20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -20\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model16')\n",
    "model16 = tmp_model; tmp_model = None\n",
    "phi16, theta16, saved_top_tokens16 = save_model_pickle('model16', model16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 02:05:06.811000] creating model\n",
      "[2016-12-20 02:05:07.624000] adding scores\n",
      "[2016-12-20 02:05:07.624000] fitting\n",
      "[2016-12-20 02:07:51.525000] outputting\n",
      "name = model17, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 10000\n",
      "ss_phi_regularizer, tau = -30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer\n",
    "                           (name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -30\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model17')\n",
    "model17 = tmp_model; tmp_model = None\n",
    "phi17, theta17, saved_top_tokens17 = save_model_pickle('model16', model17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 02:08:45.975000] creating model\n",
      "[2016-12-20 02:08:46.974000] adding scores\n",
      "[2016-12-20 02:08:46.974000] fitting\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer\n",
    "                           (name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -40\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model18')\n",
    "model18 = tmp_model; tmp_model = None\n",
    "phi18, theta18, saved_top_tokens18 = save_model_pickle('model18', model18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 14:20:54.982000] creating model\n",
      "[2016-12-20 14:20:55.797000] adding scores\n",
      "[2016-12-20 14:20:56.628000] fitting\n",
      "[2016-12-20 14:23:52.278000] outputting\n",
      "name = model19, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer\n",
    "                           (name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -40\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model19')\n",
    "model19 = tmp_model; tmp_model = None\n",
    "phi19, theta19, saved_top_tokens19 = save_model_pickle('model19', model19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 17:59:26.472000] creating model\n",
      "[2016-12-20 17:59:27.511000] adding scores\n",
      "[2016-12-20 17:59:27.526000] fitting\n",
      "[2016-12-20 18:02:19.159000] outputting\n",
      "name = model20, n_topics = 20, n_doc_passes = 5, seed_value = 200, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=200,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer\n",
    "                           (name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -40\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model20')\n",
    "model20 = tmp_model; tmp_model = None\n",
    "phi20, theta20, saved_top_tokens20 = save_model_pickle('model20', model20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 20:51:02.460000] creating model\n",
      "[2016-12-20 20:51:03.704000] adding scores\n",
      "[2016-12-20 20:51:03.705000] fitting\n",
      "[2016-12-20 20:54:04.858000] outputting\n",
      "name = model21, n_topics = 20, n_doc_passes = 5, seed_value = 300, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=300,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer\n",
    "                           (name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -40\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model21')\n",
    "model21 = tmp_model; tmp_model = None\n",
    "phi21, theta21, saved_top_tokens21 = save_model_pickle('model21', model21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 20:54:08.102000] creating model\n",
      "[2016-12-20 20:54:08.987000] adding scores\n",
      "[2016-12-20 20:54:09.002000] fitting\n",
      "[2016-12-20 20:57:36.795000] outputting\n",
      "name = model22, n_topics = 20, n_doc_passes = 5, seed_value = 400, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=400,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer\n",
    "                           (name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -40\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model22')\n",
    "model22 = tmp_model; tmp_model = None\n",
    "phi22, theta22, saved_top_tokens22 = save_model_pickle('model22', model22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 20:57:40.388000] creating model\n",
      "[2016-12-20 20:57:41.299000] adding scores\n",
      "[2016-12-20 20:57:41.303000] fitting\n",
      "[2016-12-20 21:01:39.365000] outputting\n",
      "name = model30, n_topics = 20, n_doc_passes = 5, seed_value = 200, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=200,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model30')\n",
    "model30 = tmp_model; tmp_model = None\n",
    "phi30, theta30, saved_top_tokens30 = save_model_pickle('model30', model30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 21:28:44.982000] creating model\n",
      "[2016-12-20 21:28:46.284000] adding scores\n",
      "[2016-12-20 21:28:46.295000] fitting\n",
      "[2016-12-20 21:31:51.918000] outputting\n",
      "name = model31, n_topics = 20, n_doc_passes = 5, seed_value = 300, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=300,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model31')\n",
    "model31 = tmp_model; tmp_model = None\n",
    "phi31, theta31, saved_top_tokens31 = save_model_pickle('model31', model31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-20 21:36:11.143000] creating model\n",
      "[2016-12-20 21:36:11.970000] adding scores\n",
      "[2016-12-20 21:36:11.974000] fitting\n",
      "[2016-12-20 21:39:39.768000] outputting\n",
      "name = model32, n_topics = 20, n_doc_passes = 5, seed_value = 400, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=400,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model32')\n",
    "model32 = tmp_model; tmp_model = None\n",
    "phi32, theta32, saved_top_tokens32 = save_model_pickle('model32', model32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-21 00:41:59.190000] creating model\n",
      "[2016-12-21 00:42:15.012000] adding scores\n",
      "[2016-12-21 00:42:15.027000] fitting\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=400,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model42')\n",
    "save_model_pickle('model42', tmp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=400,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer\n",
    "                           (name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model43')\n",
    "save_model_pickle('model43', tmp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_distances(dist_fun, _phi, _phi_other):\n",
    "    print '[{}] take_distances between {} columns and {} columns'.format(datetime.now(), len(_phi.columns), len(_phi_other.columns))\n",
    "    distances = pd.DataFrame(0, index = _phi.columns, columns=_phi_other.columns)\n",
    "    for idx, col in enumerate(_phi.columns):\n",
    "        print '[{}] column num {} of {}'.format(datetime.now(), idx, len(_phi.columns))\n",
    "        for idx_other, col_other in enumerate(_phi_other.columns):\n",
    "            distance = dist_fun(_phi[col], _phi_other[col_other])\n",
    "            distances.iloc[idx, idx_other] = distance\n",
    "    return distances\n",
    "def get_optimization_result_one_matrix(dist_fn, jac_dist_fn, phi, distances):\n",
    "    opt_results = {}\n",
    "    for col_idx, col_name in enumerate(phi.columns):\n",
    "        print '[{}] get_optimization_result for column {}'.format(datetime.now(), col_idx)\n",
    "        column = phi[col_name]\n",
    "        # delete col from phi\n",
    "        phi_cut = phi.drop(col_name, axis=1)\n",
    "        opt_results[col_name] = solve_optimization_problem(dist_fn, jac_dist_fn, column, col_name, phi_cut, distances)\n",
    "    return opt_results\n",
    "def get_optimization_result(dist_fn, jac_dist_fn, phi, phi_other, distances):\n",
    "    opt_results = {}\n",
    "    for col_idx, col_name in enumerate(phi.columns):\n",
    "        print '[{}] get_optimization_result for column {}'.format(datetime.now(), col_idx)        \n",
    "        column = phi[col_name]\n",
    "        opt_results[col_name] = solve_optimization_problem(dist_fn, jac_dist_fn, column, column_name, phi_other, distances)\n",
    "    return opt_results\n",
    "def solve_optimization_problem(dist_fn, jac_dist_fn, column, column_name, phi, distances, verbose=False):\n",
    "    max_iter = 50\n",
    "    phi_columns = phi.columns\n",
    "    # cut distances by phi columns \n",
    "    cut_distances = distances.loc[phi_columns]\n",
    "    # get n closest topics\n",
    "    closest_column_names = cut_distances[column_name].sort_values().head(N_CLOSEST_TOPICS).index.values\n",
    "    phi_closest = phi[closest_column_names]\n",
    "    \n",
    "    # opt solver\n",
    "    n_columns = phi_closest.shape[1] \n",
    "    bnds = [(0, 1)] * n_columns\n",
    "    constraints = cons = ({'type': 'eq', 'fun': lambda x:  np.sum(x) - 1, 'jac': lambda x: [1] * n_columns})\n",
    "    opt_fun = lambda x: dist_fn(column, phi_closest.dot(x))\n",
    "    jac_fun = lambda x: jac_dist_fn(column, phi_closest, x)\n",
    "    \n",
    "    is_optimized = False\n",
    "    it = 0\n",
    "    while (not is_optimized) and it != 4:\n",
    "        it += 1\n",
    "        init_x = np.random.uniform(0, 1, (1, n_columns))\n",
    "        init_x /= np.sum(init_x)\n",
    "        if jac_dist_fn is not None:\n",
    "            res = minimize(opt_fun, jac=jac_fun, x0=init_x, method='SLSQP', bounds=bnds, constraints=cons, options={'maxiter': max_iter, 'disp': verbose})\n",
    "        else:\n",
    "            res = minimize(opt_fun, x0=init_x, method='SLSQP', bounds=bnds, constraints=cons, options={'maxiter': max_iter, 'disp': verbose})\n",
    "        is_optimized = res.success\n",
    "    if not is_optimized:\n",
    "        print 'Not optimized' \n",
    "    res['column_names'] = phi_closest.columns\n",
    "    res['optimized_column'] = column_name\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
