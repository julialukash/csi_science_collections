{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import artm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "\n",
    "import distances_helper as dh \n",
    "import print_helper as ph\n",
    "import create_model_helper as cmh\n",
    "\n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\UCI_filtered_ngramm_trimmed_without_names\\np_01_04_choose_model\\models.txt\n",
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\..\\data\\postnauka\\UCI_collections\\UCI_filtered_ngramm_trimmed_without_names\n"
     ]
    }
   ],
   "source": [
    "config = ConfigPaths('config.cfg')\n",
    "plot_maker = PlotMaker()\n",
    "print config.models_file_name\n",
    "print config.dataset_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_file = open(config.models_file_name, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(current_dictionary, n_topics, n_doc_passes, seed_value, n_top_tokens, p_mass_threshold):    \n",
    "    print '[{}] creating model'.format(datetime.now())\n",
    "    model = artm.ARTM(num_topics=n_topics, dictionary=current_dictionary, cache_theta=True, seed=seed_value, \n",
    "                  class_ids={'ngramm': 1.0, 'author_id': 0.0, 'author': 0.0, \n",
    "                             'post_tag': 0.0, 'projects': 0.0, 'category': 0.0,\n",
    "                             'following_users': 0.0})\n",
    "    model.num_document_passes = n_doc_passes\n",
    "    add_scores_to_model(model, n_top_tokens=n_top_tokens, p_mass_threshold=p_mass_threshold)\n",
    "    return model\n",
    "def add_scores_to_model(artm_model, n_top_tokens, p_mass_threshold):\n",
    "    print '[{}] adding scores'.format(datetime.now())\n",
    "    artm_model.scores.add(artm.PerplexityScore(name='perplexity_score',\n",
    "                                      dictionary=dictionary))\n",
    "    artm_model.scores.add(artm.SparsityPhiScore(name='ss_phi_score', class_id='ngramm'))\n",
    "    artm_model.scores.add(artm.SparsityThetaScore(name='ss_theta_score'))\n",
    "    artm_model.scores.add(artm.TopicKernelScore(name='topic_kernel_score', class_id='ngramm', \n",
    "                                                probability_mass_threshold=p_mass_threshold))\n",
    "    artm_model.scores.add(artm.TopTokensScore(name='top_tokens_score', class_id='ngramm', num_tokens=n_top_tokens))\n",
    "def fit_one_model(model, _n_iterations, _model_name=''): \n",
    "    print '[{}] fitting'.format(datetime.now())\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=_n_iterations)\n",
    "    print '[{}] outputting'.format(datetime.now())\n",
    "    ph.print_artm_model(model, _model_name, _n_iterations, output_file=models_file)\n",
    "    model_pics_file_name =  path.join(config.experiment_path, _model_name)\n",
    "    plot_maker.make_tm_plots(model, model_pics_file_name)\n",
    "    model_output_file_name = path.join(config.experiment_path, _model_name + '.txt')\n",
    "    ph.print_scores(model, _model_name, _n_iterations, model_output_file_name)\n",
    "    ph.print_top_tokens(model, model_output_file_name)\n",
    "    return model\n",
    "def save_pickle_file(dists, filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'wb')\n",
    "    pickle.dump(dists, pickle_file)\n",
    "    pickle_file.close()\n",
    "def load_pickle_file(filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'rb')\n",
    "    p_file = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    return p_file\n",
    "def save_model_pickle(_model_name, _model, _save=True):\n",
    "    phi = _model.get_phi()\n",
    "    phi = phi[(phi.T != 0).any()]\n",
    "    theta = _model.get_theta()    \n",
    "    saved_top_tokens = _model.score_tracker['top_tokens_score'].last_tokens\n",
    "    if _save:\n",
    "        save_pickle_file(phi, 'phi_{}.p'.format(_model_name))\n",
    "        save_pickle_file(theta, 'theta_{}.p'.format(_model_name))\n",
    "        save_pickle_file(saved_top_tokens, 'saved_top_tokens_{}.p'.format(_model_name))\n",
    "    return phi, theta, saved_top_tokens\n",
    "def load_model_pickle(_model_name, _distance_name):\n",
    "    phi = load_pickle_file('phi_{}.p'.format(_model_name))\n",
    "    theta = load_pickle_file('theta_{}.p'.format(_model_name))\n",
    "    saved_top_tokens = load_pickle_file('saved_top_tokens_{}.p'.format(_model_name))\n",
    "    distances = load_pickle_file('{}.p'.format(_distance_name))\n",
    "    return phi, theta, saved_top_tokens, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=config.output_batches_path,\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path=config.dictionary_path + '.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-17 17:27:30.048000] creating model\n",
      "[2017-01-17 17:27:31.568000] adding scores\n",
      "[2017-01-17 17:27:31.573000] fitting\n",
      "[2017-01-17 17:28:03.547000] outputting\n",
      "name = model_100_1, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_1')\n",
    "model1 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 13:50:14.199000] creating model\n",
      "[2017-01-18 13:50:15.808000] adding scores\n",
      "[2017-01-18 13:50:15.828000] fitting\n",
      "[2017-01-18 13:50:58.132000] outputting\n",
      "name = model_100_2, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_2')\n",
    "model2 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 13:51:44.164000] creating model\n",
      "[2017-01-18 13:51:45.670000] adding scores\n",
      "[2017-01-18 13:51:45.681000] fitting\n",
      "[2017-01-18 13:52:20.233000] outputting\n",
      "name = model_100_3, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:10:41.023000] creating model\n",
      "[2017-01-18 14:10:42.603000] adding scores\n",
      "[2017-01-18 14:10:42.640000] fitting\n",
      "[2017-01-18 14:11:17.749000] outputting\n",
      "name = model_100_4, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1.5\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:13:34.646000] creating model\n",
      "[2017-01-18 14:13:36.221000] adding scores\n",
      "[2017-01-18 14:13:36.268000] fitting\n",
      "[2017-01-18 14:14:11.615000] outputting\n",
      "name = model_100_5, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -2\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_5')\n",
    "# BAD TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:15:42.702000] creating model\n",
      "[2017-01-18 14:15:44.216000] adding scores\n",
      "[2017-01-18 14:15:44.255000] fitting\n",
      "[2017-01-18 14:16:19.501000] outputting\n",
      "name = model_100_6, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:18:07.942000] creating model\n",
      "[2017-01-18 14:18:09.529000] adding scores\n",
      "[2017-01-18 14:18:09.582000] fitting\n",
      "[2017-01-18 14:18:45.054000] outputting\n",
      "name = model_100_7, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_7')\n",
    "# THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:22:17.846000] creating model\n",
      "[2017-01-18 14:22:19.331000] adding scores\n",
      "[2017-01-18 14:22:19.370000] fitting\n",
      "[2017-01-18 14:22:54.246000] outputting\n",
      "name = model_100_8, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top words better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:24:55.615000] creating model\n",
      "[2017-01-18 14:24:57.161000] adding scores\n",
      "[2017-01-18 14:24:57.201000] fitting\n",
      "[2017-01-18 14:25:32.289000] outputting\n",
      "name = model_100_9, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 5\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 5\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:33:32.630000] creating model\n",
      "[2017-01-18 14:33:33.978000] adding scores\n",
      "[2017-01-18 14:33:33.992000] fitting\n",
      "[2017-01-18 14:34:12.437000] outputting\n",
      "name = model_100_10, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:35:42.327000] creating model\n",
      "[2017-01-18 14:35:43.693000] adding scores\n",
      "[2017-01-18 14:35:43.739000] fitting\n",
      "[2017-01-18 14:36:18.083000] outputting\n",
      "name = model_100_11, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = model_100_12, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, top_tokens_score = 15, topic_kernel_score = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25, class_name='ngramm')\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20, _model_name='model_100_12')\n",
    "# THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19321, 100) (100, 3446)\n"
     ]
    }
   ],
   "source": [
    "phi3, theta3 = tmp_model.get_phi(), tmp_model.get_theta()\n",
    "print phi3.shape, theta3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2216, 100)\n"
     ]
    }
   ],
   "source": [
    "phi3_nz = phi3[(phi3.T != 0).any()]\n",
    "print phi3_nz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:41:00.920000] creating model\n",
      "[2017-01-18 14:41:02.436000] adding scores\n",
      "[2017-01-18 14:41:02.470000] fitting\n",
      "[2017-01-18 14:41:37.578000] outputting\n",
      "name = model_100_13, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:41:43.035000] creating model\n",
      "[2017-01-18 14:41:44.697000] adding scores\n",
      "[2017-01-18 14:41:44.741000] fitting\n",
      "[2017-01-18 14:42:20.007000] outputting\n",
      "name = model_100_14, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -3\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:43:35.376000] creating model\n",
      "[2017-01-18 14:43:36.783000] adding scores\n",
      "[2017-01-18 14:43:36.825000] fitting\n",
      "[2017-01-18 14:44:08.948000] outputting\n",
      "name = model_100_15, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем 3 модели: без регуляриз,  7 (норм, но у ядра размер 90) и 12 (норм и ядро размера 24). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
