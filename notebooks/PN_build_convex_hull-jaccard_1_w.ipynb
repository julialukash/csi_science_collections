{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import artm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "\n",
    "import distances_helper as dh \n",
    "import print_helper as ph\n",
    "import create_model_helper as cmh\n",
    "import build_convex_hull_helper as bchh\n",
    "import different_models as dm\n",
    "\n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\pn_model3\\np_24_02_kl\\models.txt\n"
     ]
    }
   ],
   "source": [
    "config = ConfigPaths('config_sample_m3.cfg')\n",
    "print config.models_file_name\n",
    "models_file = open(config.models_file_name, 'a')\n",
    "\n",
    "plot_maker = PlotMaker()\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=config.output_batches_path,\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path=config.dictionary_path + '.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_fn_1(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                             n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_20_m1_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_2(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                             n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_100_m2_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_3(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 300\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -20\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_20_m3_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_4(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_20_m4_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_1(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -3\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -3\n",
    "    tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_20_reg_1_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_2(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -5\n",
    "    tmp_model = fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20,                               \n",
    "                              _model_name='model_20_reg_2_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_3(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -10\n",
    "    tmp_model = fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20,             \n",
    "                              _model_name='model_20_reg_3_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_4(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer'].tau = -10 \n",
    "    tmp_model = fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20,             \n",
    "                              _model_name='model_20_reg_4_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_complex_reg_1(n_iteration):\n",
    "    n_topics = 20\n",
    "    common_topics = [u'topic_0', u'topic_1']\n",
    "    subject_topics = list(set([u'topic_{}'.format(idx) for idx in range(2, 20)]) - set(common_topics))\n",
    "    tmp_model = create_model_complex(current_dictionary=dictionary, n_topics=n_topics, n_doc_passes=5, \n",
    "        seed_value=100 + n_iteration, n_top_tokens=15, p_mass_threshold=0.25, \n",
    "        common_topics=common_topics, subject_topics=subject_topics)\n",
    "    # subject topics\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer_subject', \n",
    "        topic_names=subject_topics))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer_subject', \n",
    "        topic_names=subject_topics, class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer_subject', \n",
    "        topic_names=subject_topics, class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer_subject'].tau = -0.5\n",
    "    tmp_model.regularizers['ss_phi_regularizer_subject'].tau = -0.5\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer_subject'].tau = -10\n",
    "\n",
    "    # common topics\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer_common', \n",
    "        topic_names=subject_topics))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer_common', \n",
    "        topic_names=subject_topics, class_ids=['@default_class']))\n",
    "#     tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer_common', \n",
    "#         topic_names=subject_topics, class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer_common'].tau = 0.5\n",
    "    tmp_model.regularizers['ss_phi_regularizer_common'].tau = 0.5\n",
    "#     tmp_model.regularizers['decorrelator_phi_regularizer_common'].tau = -10\n",
    "\n",
    "    tmp_model = fit_one_model_complex(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                      tmp_model, _n_iterations=20,             \n",
    "                                      _model_name='model_20_complex_reg_1_iter_{}'.format(n_iteration))\n",
    "    return tmp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_pickle_file(dists, filename, _path=config.experiment_path):\n",
    "    pickle_filename = path.join(_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'wb')\n",
    "    pickle.dump(dists, pickle_file)\n",
    "    pickle_file.close()\n",
    "def save_model_pickle(_model_name, _model, _save=True):\n",
    "    phi = _model.get_phi()\n",
    "    phi = phi[(phi.T != 0).any()]\n",
    "    theta = _model.get_theta()    \n",
    "    saved_top_tokens = _model.score_tracker['top_tokens_score'].last_tokens\n",
    "    if _save:\n",
    "        save_pickle_file(phi, 'phi_{}.p'.format(_model_name))\n",
    "        save_pickle_file(theta, 'theta_{}.p'.format(_model_name))\n",
    "        save_pickle_file(saved_top_tokens, 'saved_top_tokens_{}.p'.format(_model_name))\n",
    "    return phi, theta, saved_top_tokens\n",
    "def load_pickle_file(filename, _path=config.experiment_path):\n",
    "    pickle_filename = path.join(_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'rb')\n",
    "    p_file = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    return p_file\n",
    "def load_model_pickle(_model_name, _distance_name=None, _path=config.experiment_path):\n",
    "    phi = load_pickle_file('phi_{}.p'.format(_model_name), _path)\n",
    "    theta = load_pickle_file('theta_{}.p'.format(_model_name), _path)\n",
    "    saved_top_tokens = load_pickle_file('saved_top_tokens_{}.p'.format(_model_name), _path)\n",
    "    distances = None\n",
    "    if _distance_name is not None:\n",
    "        distances = load_pickle_file('{}.p'.format(_distance_name), _path)\n",
    "    return phi, theta, saved_top_tokens, distances\n",
    "# now some info from run\n",
    "# distibution of topics in convex hull by iteration\n",
    "def plot_convex_hull_topics_iterations_distribution(_phi_convex_hull):\n",
    "    get_iteration_number_fn = lambda x: int(x[x.find('_', 6) + 1 : ])\n",
    "    phi_convex_hull_iteration_number = [get_iteration_number_fn(col) for col in _phi_convex_hull.columns]\n",
    "    phi_convex_hull_iteration_number = [(val, phi_convex_hull_iteration_number.count(val), 1.0 * phi_convex_hull_iteration_number.count(val) / len(phi_convex_hull_iteration_number)) for val in set(phi_convex_hull_iteration_number)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    sns.barplot([x[0] for x in phi_convex_hull_iteration_number], [x[1] for x in phi_convex_hull_iteration_number], ax=ax1)\n",
    "    ax1.set_title('Number of topics from each iteration')\n",
    "    ax1.set_xlabel('n iteration')\n",
    "\n",
    "    sns.barplot([x[0] for x in phi_convex_hull_iteration_number], [x[2] for x in phi_convex_hull_iteration_number], ax=ax2)\n",
    "    ax2.set_title('Number of topics from each iteration (%)')\n",
    "    ax2.set_xlabel('n iteration')\n",
    "def plot_convex_hull_columns_change(iterations_info):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    ax1.plot([sum([val['n_topics_to_remove']for val in it['iterations_info_filter']]) for it in iterations_info], \n",
    "             color='r', label = 'total')\n",
    "    ax1.set_title('Num columns to remove')\n",
    "    get_topic_iteration_fn = lambda x: int(x[x.rfind('_') + 1 :])\n",
    "    get_topic_filter_iteration_list_fn = lambda x, y: [get_topic_iteration_fn(topic) for topic in x].count(y)\n",
    "    n_topics_removed_from_current_iteration = [sum([get_topic_filter_iteration_list_fn(val['removed_topics'], indx) for val in it['iterations_info_filter']]) for indx, it in enumerate(iterations_info)]\n",
    "    ax1.plot(n_topics_removed_from_current_iteration, color='b', label='current iteration')\n",
    "    ax1.set_xlabel('n iteration')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot([val['phi_convex_hull_shape'][1] for val in iterations_info], color='r')\n",
    "    ax2.set_title('Num columns of convex hull')\n",
    "    ax2.set_xlabel('n iteration')\n",
    "    ax2.legend()\n",
    "def plot_opt_res_fun(iterations_filtering_info_name):\n",
    "    %matplotlib inline\n",
    "    iterations_filtering_info = load_pickle_file(iterations_filtering_info_name)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    values = [x.fun for item in iterations_filtering_info for val in item for x in val['opt_res'].values()]\n",
    "    sns.distplot(values, color='r', bins=10, ax=ax1)\n",
    "    values = [[x.fun for val in item for x in val['opt_res'].values()] for item in iterations_filtering_info]\n",
    "    for val in values:\n",
    "        sns.distplot(val, bins=10, ax=ax2)\n",
    "def plot_opt_res_fun_filtering(iterations_filtering_info_name):\n",
    "    iterations_info = load_pickle_file(iterations_filtering_info_name)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    values = [x.fun for item in iterations_info for val in item['iterations_info_filter'] for x in val['opt_res'].values()]\n",
    "    sns.distplot(values, color='r', bins=10, ax=ax1)\n",
    "    values = [[x.fun for val in item['iterations_info_filter'] for x in val['opt_res'].values()] for item in iterations_info]\n",
    "    for val in values:\n",
    "        sns.distplot(val, bins=10, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим оригинальный sample датасет (от model3), до этого скопировав в папку с batches нужные pickle файлы модели.\n",
    "Сначала провизуалируем по одной итерации каждой новой модели, а потом будем итерационно строить выпуклую оболочку для каждой модели по отдельности и затем сравнивать их. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2216, 100) (100, 3446)\n"
     ]
    }
   ],
   "source": [
    "phi_original, theta_original, saved_top_tokens_original, distances_hellinger_model_original = load_model_pickle('model3', 'distances_hellinger_model3', config.output_batches_path)\n",
    "print phi_original.shape, theta_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Запустить несколько раз с разным рандомом. Следить за тем, чтобы накапливались только независимые темы. Каждый раз смотреть. как проектируется на оригинальную матрицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phi_granularity(phi):\n",
    "    return np.mean([get_words_close_to_th_count(col)[0] for topic, col in phi.iteritems()])\n",
    "def get_words_close_to_th_count(col, global_th=0.95):\n",
    "    cur_sum, sum_count = 0, 0\n",
    "    for val in col.sort_values()[::-1]:\n",
    "        if cur_sum + val <= global_th:\n",
    "            cur_sum += val\n",
    "            sum_count += 1\n",
    "        else:\n",
    "            break\n",
    "    return sum_count + 1, cur_sum\n",
    "def get_opt_x_granularity(opt):\n",
    "    return np.mean([get_opt_x_close_to_th_count(opt_res.x)[0] for topic, opt_res in opt.iteritems()])\n",
    "def get_opt_x_close_to_th_count(opt_x, cut_th=None, global_th=0.95):\n",
    "    cur_sum, sum_count = 0, 0\n",
    "    opt_x_val = opt_x.copy()\n",
    "    if cut_th != None:\n",
    "        opt_x_val[opt_x_val < cut_th] = 0\n",
    "    for val in sorted(opt_x_val)[::-1]:\n",
    "        if val !=0 and cur_sum + val <= global_th:\n",
    "            cur_sum += val\n",
    "            sum_count += 1\n",
    "        else:\n",
    "            break\n",
    "    return sum_count + 1, cur_sum\n",
    "def get_and_plot_granularity(phi, opt_to_original, name):\n",
    "    x_count_grans = [get_opt_x_close_to_th_count(opt_res.x)[0] for topic, opt_res in opt_to_original.iteritems()]\n",
    "    x_count_grans_mean = np.mean(x_count_grans)\n",
    "    x_count_grans_th = [get_opt_x_close_to_th_count(opt_res.x, cut_th=5*1e-2)[0] for topic, opt_res in opt_to_original.iteritems()]\n",
    "    x_count_grans_mean_th = np.mean(x_count_grans_th)\n",
    "    grans = [get_words_close_to_th_count(col)[0] for topic, col in phi.iteritems()]\n",
    "    grans_mean = np.mean(grans) \n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=False, figsize=(15,5))\n",
    "    sns.distplot(x_count_grans, color='r', bins=5, ax=ax1)\n",
    "    sns.distplot(x_count_grans_th, color='b', bins=5, ax=ax1)\n",
    "    for topic, opt_res in opt_to_original.iteritems():\n",
    "        ax2.plot(sorted(opt_res.x)[::-1])\n",
    "    sns.distplot(grans, color='r', bins=5, ax=ax3)\n",
    "    title = '{} {} \\nx_count_grans_mean = {}; th={}'.format(name, phi.shape, x_count_grans_mean, x_count_grans_mean_th)\n",
    "    ax1.set_title(title)\n",
    "    title = '{} {} \\ngrans_mean = {}'.format(name, phi.shape, grans_mean)\n",
    "    ax3.set_title(title)\n",
    "\n",
    "    return x_count_grans, x_count_grans_mean, grans, grans_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_distances(phi_original, name):\n",
    "    phi_convex_hull = load_pickle_file(name)\n",
    "    distances_to_original = bchh.calculate_distances(dh.hellinger_dist, phi_convex_hull, phi_original)\n",
    "    inter_distances = bchh.calculate_distances(dh.hellinger_dist, phi_convex_hull, phi_convex_hull)\n",
    "    opt_res_to_original = bchh.get_optimization_result(dh.hellinger_dist, None, phi_convex_hull, phi_original,\n",
    "                                                       distances_to_original, n_closest_topics=N_CLOSEST_TOPICS)\n",
    "    save_pickle_file(distances_to_original, 'distances_to_original__' + name)\n",
    "    save_pickle_file(inter_distances, 'inter_distances__' + name)\n",
    "    save_pickle_file(opt_res_to_original, 'opt_res_to_original__' + name)\n",
    "    return phi_convex_hull, distances_to_original, inter_distances, opt_res_to_original\n",
    "def load_distances(name):\n",
    "    phi_convex_hull = load_pickle_file(name)\n",
    "    distances_to_original = load_pickle_file('distances_to_original__' + name)\n",
    "    inter_distances = load_pickle_file('inter_distances__' + name)\n",
    "    opt_res_to_original = load_pickle_file('opt_res_to_original__' + name)\n",
    "    return phi_convex_hull, distances_to_original, inter_distances, opt_res_to_original\n",
    "# построим распределение полученных distances\n",
    "def plot_dists(distances_to_original, inter_distances, opt_res_to_original, name):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    vals = distances_to_original.values.flatten()\n",
    "    sns.distplot(vals[vals != 0], color='r', ax = ax1, label='Inter distances')\n",
    "    vals = inter_distances.values.flatten()\n",
    "    sns.distplot(vals[vals != 0],  color='b', ax = ax1, label='Distances to original')\n",
    "    ax1.set_title(\"distances  \" + name)\n",
    "    ax1.legend()\n",
    "    sns.distplot([val.fun for val in opt_res_to_original.itervalues()], color='r', bins=10, ax=ax2)\n",
    "    ax2.set_title(\"opts  \" + name)\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi_convex_hull_20_reg_0\n"
     ]
    }
   ],
   "source": [
    "phi_convex_hull_list, distances_to_original_list, inter_distances_list, opt_res_to_original_list = [], [], [], []\n",
    "for name in names:\n",
    "    print name\n",
    "    phi_convex_hull, distances_to_original, inter_distances, opt_res_to_original = get_distances(phi_original, name)\n",
    "    phi_convex_hull_list.append(phi_convex_hull)\n",
    "    distances_to_original_list.append(distances_to_original)\n",
    "    inter_distances_list.append(inter_distances)\n",
    "    opt_res_to_original_list.append(opt_res_to_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_convex_hull_list, distances_to_original_list, inter_distances_list, opt_res_to_original_list = [], [], [], []\n",
    "for name in names:\n",
    "    phi_convex_hull, distances_to_original, inter_distances, opt_res_to_original = load_distances(name)\n",
    "    phi_convex_hull_list.append(phi_convex_hull)\n",
    "    distances_to_original_list.append(distances_to_original)\n",
    "    inter_distances_list.append(inter_distances)\n",
    "    opt_res_to_original_list.append(opt_res_to_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1\n",
    "Найти модель (1 итерация filter ch), которая\n",
    "* будет иметь гранулярность близкую к original (=55)\n",
    "* будет иметь opt_res_mean - через 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_count = 20\n",
    "names_list, phi_convex_hull_list =  [0] * models_count,  [0] * models_count\n",
    "iterations_info_list, iterations_filtering_list =  [0] * models_count, [0] * models_count\n",
    "distances_to_original_list, opt_res_to_original_list, opt_original_to_res_list = [0] * models_count, [0] * models_count, [0] * models_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_dists(opt_res_to_original, opt_original_to_res, name):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    sns.distplot([val.fun for val in opt_res_to_original.itervalues()], color='r', bins=10, ax=ax1)\n",
    "    sns.distplot([val.fun for val in opt_original_to_res.itervalues()], color='r', bins=10, ax=ax2)    \n",
    "    ax1.set_title(\"opt_res_to_original \" + name)\n",
    "    ax2.set_title(\"opt_original_to_res \" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "OPT_FUN_THRESHOLD, N_CLOSEST_TOPICS_COUNT, DIST_FN  = 0.6, 15, dh.jaccard_dist\n",
    "names_list[idx] = 'jaccard__wf__model_20_2__0.6__15_r1'\n",
    "phi_convex_hull_list[idx], iterations_info_list[idx], iterations_filtering_list[idx] = bchh \\\n",
    "                                                    .build_convex_hull_with_filtering(\n",
    "                                                     create_model_fn_2,\n",
    "                                                     bchh.get_topics_to_remove_by_opt_fun_and_distance, \n",
    "                                                     DIST_FN,\n",
    "                                                     phi_original.index,\n",
    "                                                     init_convex_hull=[], start_iteration=0,\n",
    "                                                     n_closest_topics_count=N_CLOSEST_TOPICS_COUNT,\n",
    "                                                     opt_fun_threshold=OPT_FUN_THRESHOLD,\n",
    "                                                     max_iteration=1)\n",
    "save_pickle_file(phi_convex_hull_list[idx], 'phi_convex_hull___' + names_list[idx])\n",
    "save_pickle_file(iterations_info_list[idx], 'iterations_info_list___' + names_list[idx])\n",
    "save_pickle_file(iterations_filtering_list[idx], 'iterations_filtering_list___' + names_list[idx])\n",
    "\n",
    "plot_convex_hull_topics_iterations_distribution(phi_convex_hull_list[idx])\n",
    "plot_convex_hull_columns_change(iterations_info_list[idx])\n",
    "\n",
    "distances_to_original_list[idx] = bchh.calculate_distances(DIST_FN, phi_convex_hull_list[idx], phi_original)\n",
    "opt_res_to_original_list[idx] = bchh.get_optimization_result(DIST_FN, None, phi_convex_hull_list[idx], phi_original,\n",
    "                                                   distances_to_original_list[idx], n_closest_topics=N_CLOSEST_TOPICS_COUNT)\n",
    "opt_original_to_res_list[idx] = bchh.get_optimization_result(DIST_FN, None, phi_original,  phi_convex_hull_list[idx],\n",
    "                                                   distances_to_original_list[idx].T, n_closest_topics=N_CLOSEST_TOPICS_COUNT)\n",
    "\n",
    "_,_,_,_ = get_and_plot_granularity(phi_convex_hull_list[idx], opt_res_to_original_list[idx], names_list[idx])\n",
    "_,_,_,_ = get_and_plot_granularity(phi_convex_hull_list[idx], opt_original_to_res_list[idx], names_list[idx])\n",
    "plot_dists(opt_res_to_original_list[idx], opt_original_to_res_list[idx], names_list[idx])\n",
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
